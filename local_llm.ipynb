{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell once to set up your conversation. \n",
    "\n",
    "# Set up your assistant prompt and initialize the message thread.\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Answer the user's question from the perspective of THE world's leading applicable domain knowledge expert. Process: 1. Rephrase the question \n",
    "to anticipate the user's needs. 2. Break the question into subquestions. 2. Assemble answers in a step-by-step manner organized for optimal comprehension. 3. Include Google Scholar and Google links\n",
    "formatted as follows, that is, do not include direct links to articles, instead search for related terms as follows.:\n",
    "\n",
    "- _See also:_ [Related topics for deeper understanding]\n",
    "  üìö[Research topic articles](https://scholar.google.com/scholar?q=related+terms)\n",
    "  üîç[General information](https://www.google.com/search?q=related+terms)\n",
    "\n",
    "- _You may also enjoy:_ [Topics of tangential interest]\n",
    "  üåü[Explore more](https://www.google.com/search?q=tangential+interest+terms)\n",
    "\n",
    "\"\"\"\n",
    "message_thread = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your first and subsequent questions here! Rerun this cell and the next. \n",
    "\n",
    "user_prompt = \"How can I avoid overeating?\"\n",
    "\n",
    "\n",
    "# Define the output file path for the conversation\n",
    "output_file = \"output.md\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'quantize.imatrix.chunks_count': '88', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\n",
      "llama_print_timings:        load time =   18960.15 ms\n",
      "llama_print_timings:      sample time =     156.16 ms /   300 runs   (    0.52 ms per token,  1921.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18959.86 ms /   212 tokens (   89.43 ms per token,    11.18 tokens per second)\n",
      "llama_print_timings:        eval time = 2271101.08 ms /   299 runs   ( 7595.66 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time = 2294922.43 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# The following function dynamically updates the output file as responses are generated.\n",
    "\n",
    "def clear_output_file():\n",
    "    \"\"\"\n",
    "    Clears all content from the output file by writing an empty string\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\")  # Clear the file by writing an empty string\n",
    "\n",
    "\n",
    "def write_to_file(content):\n",
    "    \"\"\"\n",
    "    Appends new content to the output file\n",
    "    Args:\n",
    "        content: Text content to append to the file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(content)\n",
    "    # If the file is not already open in VSCode, uncomment the next line\n",
    "    # os.system(f\"code {output_file}\"). \n",
    "    # Best, though, to have the file open and in preview mode. Then you see output\n",
    "    # as it is generated!\n",
    "    \n",
    "    # Add the latest prompt to the thread\n",
    "message_thread.append({\"role\": \"user\", \"content\": user_prompt}) \n",
    "\n",
    "\n",
    "# Append the Markdown content to a file\n",
    "with open(output_file, \"a\") as file:\n",
    "    file.write(f'\\n\\n _____ \\n\\n User üë©‚Äç‚öïÔ∏è: {user_prompt} \\n\\n ')\n",
    "\n",
    "llm = Llama(\n",
    "      model_path =\"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\",\n",
    "      chat_format = \"llama-3\",\n",
    "      verbose=True,\n",
    "      n_gpu_layers=0, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    "  )\n",
    "output = llm.create_chat_completion(\n",
    "        messages = message_thread,\n",
    "        max_tokens=1000,\n",
    "        stream=True,\n",
    "        stop = \"<|assistant|>\",\n",
    "        )\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        role_text = delta['role'] + \"üëΩ\" + \": \"\n",
    "        # print(role_text, end='')\n",
    "        text += role_text\n",
    "        write_to_file(role_text)\n",
    "    elif 'content' in delta:\n",
    "        content_text = delta['content']\n",
    "        # print(content_text, end='')\n",
    "        text += content_text\n",
    "        write_to_file(content_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the context window is big enough, add the latest response to the thread. You can then go back to update the user prompt and send again.\n",
    "# message_thread.append({\"role\": \"system\", \"content\": text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, if you want to clear your output!\n",
    "\n",
    "\n",
    "clear_output_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Students will need to fill in the critical functions\n",
    "# Hints are provided within the comments.\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system interface - for file and path operations\n",
    "import os\n",
    "\n",
    "# NumPy - Scientific computing library for array operations and numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# PDF processing library - for reading and extracting text from PDF files\n",
    "import PyPDF2\n",
    "\n",
    "# Sentence Transformers - for creating semantic embeddings from text\n",
    "# (Note: This import appears unused in the current code as you're using Llama for embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "# Facebook AI Similarity Search (FAISS) - for efficient similarity search and clustering of dense vectors\n",
    "# Used here for storing and querying text embeddings\n",
    "import faiss\n",
    "\n",
    "# Python bindings for the llama.cpp library - provides access to Llama language models\n",
    "# Used for text generation and creating embeddings\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_file = \"Sample Patient 1.pdf\" \n",
    "\n",
    "# Create a PDF reader object to read the file\n",
    "pdf_reader = PyPDF2.PdfReader(open(pdf_file, \"rb\"))\n",
    "pdf_text = \"\"\n",
    "\n",
    "# Iterate through each page of the PDF\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract text from the current page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text()\n",
    "    # If text was successfully extracted, add it to our accumulated text\n",
    "    if page_text:\n",
    "        pdf_text += page_text + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 1\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500, chunk_overlap=50) -> list:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks for processing\n",
    "    Args:\n",
    "        text: Input text to be chunked\n",
    "        chunk_size: Number of words per chunk\n",
    "        chunk_overlap: Number of overlapping words between chunks\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    HINTS:\n",
    "      - Split the input text into a list of words.\n",
    "      - Use a while loop to keep creating chunks until you've processed all words.\n",
    "      - For each chunk:\n",
    "        1. Calculate the end position based on `chunk_size`.\n",
    "        2. Join the words from `start` to `end` into a single string.\n",
    "        3. Append this chunk to a list.\n",
    "        4. Adjust `start` by subtracting `chunk_overlap` to ensure overlapping words.\n",
    "      - Make sure `start` never goes below zero.\n",
    "      - Return the list of chunks.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Split the input text into chunks\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    # Creating chunks using the specified size and overlap\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = max(0, end - chunk_overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Process the PDF text into chunks and print the total number of chunks created\n",
    "text_chunks = chunk_text(pdf_text, chunk_size=500, chunk_overlap=50)\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded with embedding support.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\"\n",
    "\n",
    "llm_for_embeddings = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model file\n",
    "    n_ctx=2048,              # Context window size\n",
    "    embedding=True,          # Enable embedding generation\n",
    "    verbose=False            # Disable verbose output\n",
    ")\n",
    "\n",
    "print(\"Llama model loaded with embedding support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 395, 4096)\n"
     ]
    }
   ],
   "source": [
    "def llama_embed_text(text) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for input text using the Llama model.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to embed.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array containing the mean embedding vector.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for the input text using the Llama model\n",
    "    embedding_result = llm_for_embeddings.create_embedding(text)\n",
    "\n",
    "    # Extract the embeddings from the result\n",
    "    if 'data' in embedding_result:\n",
    "        # Assuming embedding_result['data'] is a list of dictionaries with key 'embedding'\n",
    "        token_embeddings = [\n",
    "            entry['embedding'] for entry in embedding_result['data']\n",
    "        ]\n",
    "\n",
    "        # Convert the token embeddings to a numpy array\n",
    "        embeddings_array = np.array(token_embeddings)\n",
    "\n",
    "        # Log the shape for debugging\n",
    "        print(f\"Raw token embeddings shape: {embeddings_array.shape}\")\n",
    "\n",
    "        # Validate and handle shape (batch_size, token_count, embedding_size)\n",
    "        if len(embeddings_array.shape) == 3 and embeddings_array.shape[2] == 4096:\n",
    "            # Calculate the mean over the token dimension (axis=1)\n",
    "            mean_embedding = np.mean(embeddings_array[0], axis=0)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected token embeddings shape {embeddings_array.shape}. \"\n",
    "                \"Ensure it is (batch_size, token_count, embedding_size).\"\n",
    "            )\n",
    "\n",
    "        # Return the mean vector\n",
    "        return mean_embedding.astype(\"float32\")\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected format for embedding result. Missing 'data' key.\")\n",
    "\n",
    "\n",
    "\n",
    "# Process embeddings for all text chunks\n",
    "all_embeddings = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    # Generate embeddings for the current chunk\n",
    "    embedding_vector = llama_embed_text(chunk)\n",
    "\n",
    "    # Append the resulting embedding vector to the all_embeddings list\n",
    "    all_embeddings.append(embedding_vector)\n",
    "\n",
    "# Stack all embeddings into a single matrix of shape (N, 4096)\n",
    "embeddings_matrix = np.vstack(all_embeddings).astype(\"float32\")\n",
    "\n",
    "# Initialize FAISS index for similarity search\n",
    "index = faiss.IndexFlatL2(4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FAISS index using L2 (Euclidean) distance metric\n",
    "# embeddings_matrix.shape[1] specifies the dimensionality of our vectors (4096 in this case)\n",
    "index = faiss.IndexFlatL2(embeddings_matrix.shape[1])\n",
    "\n",
    "# Add all our document embeddings to the index\n",
    "index.add(embeddings_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 8, 4096)\n",
      "--- Result 1 (distance 10810.9570) ---\n",
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n",
      "\n",
      "--- Result 2 (distance 340282346638528859811704183484516925440.0000) ---\n",
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n",
      "\n",
      "--- Result 3 (distance 340282346638528859811704183484516925440.0000) ---\n",
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_similar_chunks(query, k=3) -> list:\n",
    "    \"\"\"\n",
    "    Searches for text chunks similar to the query using FAISS.\n",
    "\n",
    "    Args:\n",
    "        query: Search query text.\n",
    "        k: Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing (text_chunk, distance).\n",
    "    \"\"\"\n",
    "    # Generate the query embedding using the same function\n",
    "    query_emb = llama_embed_text(query)\n",
    "\n",
    "    # Ensure the query embedding is float32 and reshaped\n",
    "    query_emb = query_emb.astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "    # Validate the query embedding dimensions match the FAISS index dimensions\n",
    "    if query_emb.shape[1] != index.d:\n",
    "        raise ValueError(\n",
    "            f\"Query embedding dimension ({query_emb.shape[1]}) does not match FAISS index dimension ({index.d}).\"\n",
    "        )\n",
    "\n",
    "    # Perform the similarity search\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "\n",
    "    # Retrieve the corresponding text chunks and distances\n",
    "    similar_chunks = [\n",
    "        (text_chunks[idx], distances[0][i]) for i, idx in enumerate(indices[0])\n",
    "    ]\n",
    "\n",
    "    return similar_chunks\n",
    "\n",
    "\n",
    "# Example usage of the search function\n",
    "test_query = \"How can I avoid overeating?\"\n",
    "search_results = search_similar_chunks(test_query, k=3)\n",
    "\n",
    "# Print each result with its similarity score\n",
    "for i, (chunk, dist) in enumerate(search_results):\n",
    "    print(f\"--- Result {i+1} (distance {dist:.4f}) ---\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'quantize.imatrix.chunks_count': '88', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Llama model for chat interactions\n",
    "llm_for_chat = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model weights file\n",
    "    n_ctx=2048,              # Maximum context window size (in tokens)\n",
    "    verbose=True             # Enable detailed logging output\n",
    ")\n",
    "\n",
    "def run_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs the Llama model with a simple prompt\n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "    Returns:\n",
    "        Generated response from the model\n",
    "    \"\"\"\n",
    "    # Create the messages list with system and user roles\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Call llm_for_chat.create_chat_completion() with the messages\n",
    "    response = llm_for_chat.create_chat_completion(messages=messages)\n",
    "\n",
    "    # Extract and return the text from the response\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_with_pdf_knowledge(user_query, k=3):\n",
    "    \"\"\"\n",
    "    Runs the Llama model with context from similar PDF chunks to provide informed answers.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user's question to be answered.\n",
    "        k (int): Number of similar chunks to retrieve from the PDF (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response incorporating knowledge from the PDF context.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve  k similar chunks from the PDF\n",
    "    similar_chunks = search_similar_chunks(user_query, k)\n",
    "    \n",
    "    # Step 2: Extract the text component from each tuple\n",
    "    chunk_texts = [chunk[0] for chunk in similar_chunks]\n",
    "    \n",
    "    # Step 3: Combine the retrieved chunk texts into a single 'context' string\n",
    "    context = \"\\n\\n\".join(chunk_texts)\n",
    "    \n",
    "    # Step 4: Construct a prompt that includes the PDF context and the user's query\n",
    "    prompt = (\n",
    "        f\"You are an assistant knowledgeable in the following context:\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Based on this context, answer the following question:\\n\"\n",
    "        f\"{user_query}\"\n",
    "    )\n",
    "    \n",
    "    # Step 5: Pass the constructed prompt to run_llm to get the response\n",
    "    response = run_llm(prompt)\n",
    "    \n",
    "    # Step 6: Return the final response text\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 12, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      77.84 ms /   139 runs   (    0.56 ms per token,  1785.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   89986.38 ms /  1235 tokens (   72.86 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:        eval time = 1044172.28 ms /   138 runs   ( 7566.47 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time = 1136273.81 ms /  1373 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Whats is the avg cgm values for my patient?\n",
      "A: Unfortunately, there is no Continuous Glucose Monitoring (CGM) data provided in the given clinical profile. The only available glucose measurements are:\n",
      "\n",
      "* Last HbA1c: 8.3%\n",
      "* Fasting Glucose: 160 mg/dL\n",
      "\n",
      "To estimate average CGM values, we would need more frequent and continuous glucose readings over a longer period. However, based on the provided data, we can infer that the patient's blood sugar levels are above target (HbA1c > 7%) and fasting glucose is slightly elevated.\n",
      "\n",
      "If you'd like to discuss ways to improve glycemic control or develop a plan for your patient, I'm here to help!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      76.66 ms /   130 runs   (    0.59 ms per token,  1695.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7218.28 ms /    16 tokens (  451.14 ms per token,     2.22 tokens per second)\n",
      "llama_print_timings:        eval time =  965335.15 ms /   129 runs   ( 7483.22 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time =  974511.10 ms /   145 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Whats is the avg cgm values for my patient?\n",
      "A: I'd be happy to help! However, I need more information from you. Could you please provide me with your patient's name or any relevant medical history so that I can better assist you?\n",
      "\n",
      "Also, could you specify what type of CGM (Continuous Glucose Monitoring) values you are referring to? Are you looking for average glucose levels in a specific time frame, such as daily averages or weekly averages? Or are you looking for average glucose levels during a particular period, such as during exercise or at night?\n",
      "\n",
      "Once I have more information from you, I'll do my best to provide you with the average CGM values for your patient.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"Whats is the avg cgm values for my patient?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 8, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      24.64 ms /    52 runs   (    0.47 ms per token,  2109.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   69727.31 ms /  1214 tokens (   57.44 ms per token,    17.41 tokens per second)\n",
      "llama_print_timings:        eval time =  365997.66 ms /    51 runs   ( 7176.42 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  436489.87 ms /  1265 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's ID?\n",
      "A: There is no ID mentioned in the provided clinical profile. The information provided only includes demographic data, medical history, medications, comorbidities, social history, lifestyle factors, and a patient health assessment questionnaire. There is no identification number or ID mentioned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      28.72 ms /    60 runs   (    0.48 ms per token,  2088.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6683.13 ms /    12 tokens (  556.93 ms per token,     1.80 tokens per second)\n",
      "llama_print_timings:        eval time =  417125.63 ms /    59 runs   ( 7069.93 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  424643.37 ms /    71 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's ID?\n",
      "A: I'm happy to help! However, I'm a new AI model and don't have any information about a specific patient. Could you please provide more context or clarify which patient you're referring to? If you'd like to create a new patient record, I can assist with that as well!\n"
     ]
    }
   ],
   "source": [
    "new_query = \"What is the patient's ID?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 11, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      28.41 ms /    60 runs   (    0.47 ms per token,  2111.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   75045.91 ms /  1217 tokens (   61.66 ms per token,    16.22 tokens per second)\n",
      "llama_print_timings:        eval time =  430466.50 ms /    59 runs   ( 7296.04 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  506401.59 ms /  1276 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's last recorded blood pressure?\n",
      "A: The patient's medical history does not mention their last recorded blood pressure. However, it does mention that they have hypertension, which is controlled with medication (Lisinopril 10 mg once daily). It also mentions that their blood pressure is under control, but the exact value is not specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      32.22 ms /    63 runs   (    0.51 ms per token,  1955.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6852.16 ms /    15 tokens (  456.81 ms per token,     2.19 tokens per second)\n",
      "llama_print_timings:        eval time =  444814.43 ms /    62 runs   ( 7174.43 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  452548.01 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's last recorded blood pressure?\n",
      "A: I'm happy to help! However, I'm a large language model, I don't have access to any specific patient information or medical records. If you're looking for the patient's last recorded blood pressure, I recommend checking their medical chart or contacting their healthcare provider directly for accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"What is the patient's last recorded blood pressure?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 8, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      11.43 ms /    25 runs   (    0.46 ms per token,  2186.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72648.66 ms /  1214 tokens (   59.84 ms per token,    16.71 tokens per second)\n",
      "llama_print_timings:        eval time =  173760.00 ms /    24 runs   ( 7240.00 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  246758.51 ms /  1238 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's BMI?\n",
      "A: According to the clinical profile, the patient's BMI is 31 kg/m¬≤, which falls into the obese category.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      29.99 ms /    57 runs   (    0.53 ms per token,  1900.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6583.45 ms /    12 tokens (  548.62 ms per token,     1.82 tokens per second)\n",
      "llama_print_timings:        eval time =  407896.56 ms /    56 runs   ( 7283.87 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  415305.87 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's BMI?\n",
      "A: I'm happy to help! However, I don't have any information about a specific patient. Could you please provide me with more context or details about the patient, such as their height and weight? That way, I can calculate their BMI (Body Mass Index) for you.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"What is the patient's BMI?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 7, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      31.14 ms /    66 runs   (    0.47 ms per token,  2119.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   71401.72 ms /  1213 tokens (   58.86 ms per token,    16.99 tokens per second)\n",
      "llama_print_timings:        eval time =  470100.29 ms /    65 runs   ( 7232.31 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  542423.65 ms /  1278 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what is today's date?\n",
      "A: I apologize, but there is no information provided about the current date in the given clinical profile. The patient's medical history, medications, and lifestyle factors are all documented, but there is no mention of a specific date or time frame. If you have any other questions or concerns, I'll do my best to assist you!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      41.78 ms /   118 runs   (    0.35 ms per token,  2824.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6654.77 ms /    11 tokens (  604.98 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:        eval time =  788741.70 ms /   117 runs   ( 6741.38 ms per token,     0.15 tokens per second)\n",
      "llama_print_timings:       total time =  796678.92 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what is today's date?\n",
      "A: I'm happy to help! However, I'm a large language model, I don't have real-time access to the current date and time. But I can suggest some ways for you to find out what today's date is!\n",
      "\n",
      "You can check your phone or computer's clock, or look at a calendar app on your device. Alternatively, you can search online for \"what is today's date\" or check a reliable news website or weather forecast website that displays the current date.\n",
      "\n",
      "If you're in a specific location, you can also ask someone around you what today's date is!\n"
     ]
    }
   ],
   "source": [
    "new_query = \"what is today's date?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw token embeddings shape: (1, 8, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =       8.23 ms /    27 runs   (    0.30 ms per token,  3281.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55951.65 ms /  1214 tokens (   46.09 ms per token,    21.70 tokens per second)\n",
      "llama_print_timings:        eval time =  168981.65 ms /    26 runs   ( 6499.29 ms per token,     0.15 tokens per second)\n",
      "llama_print_timings:       total time =  225215.13 ms /  1240 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Is the patient feeling numbness?\n",
      "A: According to the clinical profile, yes, the patient is experiencing tingling and numbness in their feet due to diabetic neuropathy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   35326.74 ms\n",
      "llama_print_timings:      sample time =      41.12 ms /   123 runs   (    0.33 ms per token,  2991.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6269.05 ms /    12 tokens (  522.42 ms per token,     1.91 tokens per second)\n",
      "llama_print_timings:        eval time =  798644.63 ms /   122 runs   ( 6546.27 ms per token,     0.15 tokens per second)\n",
      "llama_print_timings:       total time =  806132.46 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Is the patient feeling numbness?\n",
      "A: I'm happy to help! As a helpful assistant, I don't have direct access to the patient's medical information or physical presence. However, I can provide general information and guidance on numbness.\n",
      "\n",
      "Numbness is a common symptom that can be caused by various factors such as nerve damage, compression, or inflammation. If you're concerned about someone's numbness, it's essential to consult with a healthcare professional for an accurate diagnosis and proper treatment.\n",
      "\n",
      "Can you please provide more context or information about the patient's symptoms? For instance, where exactly are they feeling numbness, and when did it start?\n"
     ]
    }
   ],
   "source": [
    "new_query = \"Is the patient feeling numbness?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
